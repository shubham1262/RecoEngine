{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashant.singh/anaconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-10 00:32:31.221867: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 4.70 MiB (download: 4.70 MiB, generated: 32.41 MiB, total: 37.10 MiB) to /Users/prashant.singh/tensorflow_datasets/movielens/100k-ratings/0.1.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ url]\n",
      "Extraction completed...: 100%|██████████| 23/23 [00:05<00:00,  4.04 file/s]\n",
      "Dl Size...: 100%|██████████| 4/4 [00:05<00:00,  1.43s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.70s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to /Users/prashant.singh/tensorflow_datasets/movielens/100k-ratings/0.1.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 00:33:14.774918: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-04-10 00:33:14.801024: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning categorical features into embeddings\n",
    "-Taking raw categorical features and turning them into embeddings is normally a two-step process:\n",
    "\n",
    "- Firstly, we need to translate the raw values into a range of contiguous integers, normally by building a mapping (called a \"vocabulary\") that maps raw values (\"Star Wars\") to integers (say, 15).\n",
    "- Secondly, we need to take these integers and turn them into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "movie_title_lookup = tf.keras.layers.StringLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/prashant.singh/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/prashant.singh/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['[UNK]', 'Star Wars (1977)', 'Contact (1997)']\n"
     ]
    }
   ],
   "source": [
    "movie_title_lookup.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "print(f\"Vocabulary: {movie_title_lookup.get_vocabulary()[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 1, 58])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_lookup([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1665"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title_lookup.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "movie_title_embedding = tf.keras.layers.Embedding(\n",
    "    # Let's use the explicit vocabulary lookup.\n",
    "    input_dim=movie_title_lookup.vocab_size(),\n",
    "    output_dim=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can put the two together into a single layer which takes raw text in and yields embeddings.\n",
    "movie_title_model = tf.keras.Sequential([movie_title_lookup, movie_title_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['Star Wars (1977)']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=['Star Wars (1977)']. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[ 0.03568078, -0.02238535,  0.02788961,  0.04128648, -0.00082426,\n",
       "         0.02608849, -0.02044696,  0.04945472,  0.01239889, -0.02821351,\n",
       "         0.00519379, -0.00533354,  0.03785529, -0.01707122, -0.04420501,\n",
       "        -0.02940728, -0.01867411,  0.03792833, -0.01515732,  0.0156895 ,\n",
       "         0.04230876,  0.02508423,  0.03938014,  0.03313514, -0.03553696,\n",
       "        -0.03068694, -0.04309207, -0.04778457,  0.01527877, -0.04624236,\n",
       "        -0.04745284, -0.00493522]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings\n",
    "movie_title_model([\"Star Wars (1977)\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can do the same with user embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    }
   ],
   "source": [
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32)\n",
    "\n",
    "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 879024327.\n",
      "Timestamp: 875654590.\n",
      "Timestamp: 882075110.\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "  print(f\"Timestamp: {x['timestamp']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized timestamp: [-0.84293723].\n",
      "Normalized timestamp: [-1.4735204].\n",
      "Normalized timestamp: [-0.27203268].\n"
     ]
    }
   ],
   "source": [
    "timestamp_normalization = tf.keras.layers.Normalization(\n",
    "    axis=None\n",
    ")\n",
    "timestamp_normalization.adapt(ratings.map(lambda x: x[\"timestamp\"]).batch(1024))\n",
    "\n",
    "for x in ratings.take(3).as_numpy_iterator():\n",
    "  print(f\"Normalized timestamp: {timestamp_normalization(x['timestamp'])}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: [8.74724710e+08 8.74743291e+08 8.74761871e+08]\n"
     ]
    }
   ],
   "source": [
    "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000)\n",
    "\n",
    "print(f\"Buckets: {timestamp_buckets[:3]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text features\n",
    "- The first transformation we need to apply to text is tokenization (splitting into constituent words or word-pieces), followed by vocabulary learning, followed by an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "# And then pass it through embedding layer, each word will have it's own embeddings, so we do global average pooling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User model\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        user_id_lookup,\n",
    "        tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32),\n",
    "    ])\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "      tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32)\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
    "        axis=None\n",
    "    )\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1))\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [ 0.04071018 -0.03020514  0.00565251]\n"
     ]
    }
   ],
   "source": [
    "user_model = UserModel()\n",
    "\n",
    "user_model.normalized_timestamp.adapt(\n",
    "    ratings.map(lambda x: x[\"timestamp\"]).batch(128))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "  print(f\"Computed representations: {user_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie Model\n",
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      movie_title_lookup,\n",
    "      tf.keras.layers.Embedding(movie_title_lookup.vocab_size(), 32)\n",
    "    ])\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.TextVectorization(max_tokens=max_tokens),\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      # We average the embedding of individual words to get one embedding vector\n",
    "      # per title.\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(inputs[\"movie_title\"]),\n",
    "        self.title_text_embedding(inputs[\"movie_title\"]),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed representations: [-0.00499157 -0.00371711 -0.0391738 ]\n"
     ]
    }
   ],
   "source": [
    "movie_model = MovieModel()\n",
    "\n",
    "movie_model.title_text_embedding.layers[0].adapt(\n",
    "    ratings.map(lambda x: x[\"movie_title\"]))\n",
    "\n",
    "for row in ratings.batch(1).take(1):\n",
    "  print(f\"Computed representations: {movie_model(row)[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
